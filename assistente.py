from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.prompts import PromptTemplate
from langchain.memory import ConversationBufferWindowMemory
from langchain.chains.conversation.base import ConversationChain
from bd import bd_pool
from dotenv import load_dotenv

load_dotenv()
memoria = ConversationBufferWindowMemory(k=5)

def get_llm():
    llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash")
    return llm
 
def assistente_geral(question):  
    if question[0] == "0" or question[0] == "1":
        resposta = gerar_resposta_inicio(question)
        return resposta
    resp = tipo_pergunta(question)
    tipo = resp.strip()  
    
    if tipo == "CONVERSA":
        resposta = gerar_resposta(question)
        return resposta
    elif tipo == "CONFIGURACAO":
        resposta = gerar_resp_confg(question)
        return resposta
    else:
        queryResposta = gerar_query(question)
        try:
            prompt = PromptTemplate.from_template(
    template="""
        Voc√™ √© um assistente virtual para os funcion√°rios de uma loja da Fiat, com acesso a um banco de dados. 
        Seu objetivo √© analisar o hist√≥rico de conversas fornecido e o input no formato (pergunta//resultado da query) do funcion√°rio, 
        para formular uma resposta clara, estruturada e otimizada para o Telegram.
        Regras de formata√ß√£o:
        - Divida a resposta em se√ß√µes claras e organizadas.
        - Use marcadores e listas para facilitar a leitura.
        - Nao coloque asteriscos para marcar, pois n√£o √© compat√≠vel com o Telegram.
        - Converta n√∫meros de telefone em links clic√°veis para o WhatsApp (exemplo: https://wa.me/55[telefone]).
        - Use emojis para destacar informa√ß√µes importantes (exemplo: üìû para telefone, üìÖ para datas, etc.).
        - Nunca inclua a query SQL ou detalhes t√©cnicos na resposta.
        Utilize as informa√ß√µes dispon√≠veis na sess√£o apenas se forem relevantes para responder √† solicita√ß√£o.
        ### Hist√≥rico de Conversas:  
        {history}
        ### Pergunta do Funcion√°rio e Resultado da Query:  
        {input}
        - Se o input vier ALTERA/DELETA, diga ooperacoes de altera√ß√£o e exclus√£o sao realizadas somente no site https://clientesfiat.onrender.com/
        Formule uma resposta estruturada e direta que atenda √† solicita√ß√£o do funcion√°rio de forma eficiente.
    """
)      
            chain = ConversationChain(
                llm=get_llm(),
                memory=memoria,
                prompt=prompt
            ) 
            pergunta = f'{question}//{queryResposta}'
            resposta = chain.predict(input=pergunta)    
            return resposta
        except Exception as e:
            return f"Erro ao acessar a LLM: {str(e)}"
    
def gerar_query(question):
    try:
        prompt = PromptTemplate.from_template(
    template="""
        Gere apenas uma query SQL, sem usar markdown, para o banco de dados PostgreSQL, baseada na pergunta abaixo:
        {input}
        Contexto da conversa:
        {history}
        Estrutura das tabelas:
        - **Funcionario**: (id_func, cpf, nome, senha)
        - **Cliente**: (id_clien, id_funcionario, nome, cpf, telefone, nome_conjuge, data_nascimento, data_casamento, data_aniver_conjuge, data_compra, revisoes_pendentes, data_ultima_revisao)
        - **Filho**: (id_filho, id_cliente, nome, data_nascimento)
        Notas importantes:
        - Se for um query que **altera** ou **deleta** alguma coisa do BD, retorne apenas: ALTERA/DELETA
        - Ignore o conte√∫do da sess√£o, mas inclua-o na resposta exatamente como recebido na pergunta.
        - Use **apenas** as tabelas fornecidas.
        - Considere que clientes solteiros possuem `nome_conjuge = 'Solteiro(a)'`.
        - Se um CPF for informado no formato `00000000000`, transforme-o para o formato `000.000.000-00`.
    """
)

        
        chain = ConversationChain(
            llm=get_llm(),
            memory=memoria,
            prompt=prompt
        )
        
        resposta = chain.predict(input=question)
        
        query = resposta.strip()
        print(query)
        connect = bd_pool.getconn()
        cursor = connect.cursor()
        cursor.execute(query)
        result = cursor.fetchall()
        return result
    except Exception as e:
        return f"Error: {str(e)}"
    finally:
        if cursor:
            cursor.close()
        if connect:
            bd_pool.putconn(connect)

def tipo_pergunta(question):
    prompt = PromptTemplate.from_template(
    template="""
        Regras de resposta:
        - Se a pergunta aparentar requerer uma consulta ao banco de dados (BD), retorne apenas: CONSULTA.
        - Se a pergunta n√£o precisar de uma consulta ou se a informa√ß√£o na sess√£o responder √† quest√£o, retorne apenas: CONVERSA.
        - Se o usu√°rio desejar trocar de usu√°rio ou ajustar o hor√°rio das verifica√ß√µes/alertas di√°rios, retorne apenas: CONFIGURACAO.
        - Ignore o conte√∫do da sess√£o, mas inclua-o na resposta exatamente como recebido na pergunta.
        Contexto da conversa:
        {history}
        Pergunta do usu√°rio:
        {input}
    """
)

    chain = ConversationChain(
        llm=get_llm(),
        memory=memoria,
        prompt=prompt,
        verbose=True
    )
    resposta = chain.predict(input=question)
    return resposta

def gerar_resposta(question):
    prompt = PromptTemplate.from_template(
    template="""
        Voc√™ √© um assistente virtual exclusivo para funcion√°rios de uma loja da Fiat. 
        Sua tarefa √© ser gentil e eficiente, ajudando no que for necess√°rio.
        Diretrizes:
        - Utilize as informa√ß√µes da sess√£o apenas se forem relevantes para a solicita√ß√£o do usu√°rio.
        - Nunca compartilhe diretamente o conte√∫do da sess√£o.
        - Nunca inclua queries SQL na resposta ao usu√°rio.
        - Mantenha suas respostas diretas e objetivas, sem perder a cordialidade.
        Contexto da conversa:
        {history}
        Pergunta do funcion√°rio:
        {input}
    """
)

    
    chain = ConversationChain(
        llm=get_llm(),
        memory=memoria,
        prompt=prompt,
    )
    
    resposta = chain.predict(input=question)  
    return resposta

def formatar_dados(dados):
    prompt = PromptTemplate.from_template(
    template="""
        {history}
        Todos os dias voc√™ recebe uma lista de alertas para verifica√ß√£o.
        Regras de formata√ß√£o:
        - Estruture as informa√ß√µes em se√ß√µes claras e organizadas para facilitar a leitura no Telegram.
        - Nao coloque asteriscos para marcar, pois n√£o √© compat√≠vel com o Telegram.
        - Utilize emojis para destacar informa√ß√µes importantes (exemplo: üë§ para nomes, üìû para telefones, üìÖ para datas, etc.).
        - Converta n√∫meros de telefone em links clic√°veis para o WhatsApp (exemplo: https://wa.me/55[phone]).
        - Caso n√£o haja alertas, responda apenas: "Hoje n√£o houve alertas de verifica√ß√£o."
        - Liste cada alerta de forma separada, numerando ou utilizando marcadores para identifica√ß√£o.

        Dados recebidos (um dicion√°rio com tipos de verifica√ß√£o como chaves e respostas como valores):
        {input}
    """
)
    chain = ConversationChain(
        llm=get_llm(),
        memory=memoria,
        prompt=prompt
    )
    resposta = chain.predict(input=dados)
    return resposta

def gerar_resp_confg(question):
    prompt = PromptTemplate.from_template(
    template="""
    Sempre o usu√°rio quiser saber sobre, instrua da seguinte forma:
    - Para trocar de usu√°rio, clique em /start.
    - Para alterar o hor√°rio, clique em /definirHorario.
    Utilize as informa√ß√µes da sess√£o apenas se forem solicitadas pelo usu√°rio e relevantes para a resposta.
    Regras:
    - Nao passe a sess√£o diretamente, apenas as informacoes nela.
    - Nunca inclua queries SQL diretamente na resposta ao usu√°rio.
    - Responda de forma clara, natural e amig√°vel, considerando o contexto abaixo.
    Contexto da conversa:
    {history}
    Pergunta do usu√°rio:
    {input}
    """
)
    
    chain = ConversationChain(
        llm=get_llm(),
        memory=memoria,
        prompt=prompt
    )
    
    resposta = chain.predict(input=question)  
    return resposta

def gerar_resposta_inicio(question):
    prompt = PromptTemplate.from_template(
    template="""
    O input vem no formato "0" ou "1" // sessao
    Voc√™ √© um assistente virtual para uma loja da Fiat, exclusivamente para funcion√°rios. Seja cort√™s e direto.
    Se receber 1, o CPF est√° validado e o funcion√°rio est√° cadastrado. Diga as boas-vindas com o nome do funcion√°rio icluso sa sessao e informe que √© um assistente virtual.
    Se receber 0, pe√ßa para o funcion√°rio informar o CPF corretamente.
    Utilize a sess√£o (informa√ß√µes do usu√°rio) para personalizar a resposta, se necess√°rio.Nunca passe a sess√£o diretamente, apenas as informa√ß√µes nela.
    {history}
    human:
    {input}
    """
)    
    chain = ConversationChain(
        llm=get_llm(),
        memory=memoria,
        prompt=prompt
    ) 
    resposta = chain.predict(input=question)  
    return resposta

if __name__ == "__main__":
    pass